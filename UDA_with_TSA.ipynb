{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMWWTL9GTDN9Ws6+npAAKlG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RajeswariKumaran/SSLMethodsAnalysis/blob/main/UDA_with_TSA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#revamped code"
      ],
      "metadata": {
        "id": "L_fpBpLlCPJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_tsa(logits, targets, epoch, total_epochs, schedule='linear'):\n",
        "    probs = torch.softmax(logits, dim=1)\n",
        "    correct_class_probs = probs[range(len(targets)), targets]\n",
        "\n",
        "    # Define the threshold schedule\n",
        "    step_ratio = epoch / total_epochs\n",
        "    if schedule == 'linear':\n",
        "        threshold = step_ratio\n",
        "    elif schedule == 'exp':\n",
        "        threshold = np.exp((step_ratio - 1) * 5)\n",
        "    elif schedule == 'log':\n",
        "        threshold = 1 - np.exp(-step_ratio * 5)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid TSA schedule\")\n",
        "\n",
        "    # TSA threshold (e.g., linear from 1/num_classes to 1.0)\n",
        "    tsa_thresh = threshold * (1 - 1 / logits.shape[1]) + 1 / logits.shape[1]\n",
        "\n",
        "    # Only compute loss for examples below the threshold\n",
        "    loss_mask = (correct_class_probs < tsa_thresh).float()\n",
        "\n",
        "    ce_loss = nn.CrossEntropyLoss(reduction='none')\n",
        "    losses = ce_loss(logits, targets)\n",
        "\n",
        "    # Apply TSA mask\n",
        "    masked_loss = (losses * loss_mask).mean()\n",
        "    return masked_loss\n"
      ],
      "metadata": {
        "id": "4GIZuHuF1BOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "# ---------- Cutout Augmentation ----------\n",
        "class Cutout:\n",
        "    def __init__(self, mask_size, p=1.0, mask_color=0):\n",
        "        self.mask_size = mask_size\n",
        "        self.p = p\n",
        "        self.mask_color = mask_color\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if random.random() > self.p:\n",
        "            return img\n",
        "        w, h = img.size\n",
        "        mask_size_half = self.mask_size // 2\n",
        "        cx = random.randint(mask_size_half, w - mask_size_half)\n",
        "        cy = random.randint(mask_size_half, h - mask_size_half)\n",
        "        x1 = cx - mask_size_half\n",
        "        y1 = cy - mask_size_half\n",
        "        x2 = cx + mask_size_half\n",
        "        y2 = cy + mask_size_half\n",
        "        img = img.copy()\n",
        "        img.paste(self.mask_color, (x1, y1, x2, y2))\n",
        "        return img\n",
        "\n",
        "# ---------- Simple CNN Model ----------\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(8*8*128, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# ---------- Strong Augmentation Pipeline ----------\n",
        "class StrongTransform:\n",
        "    def __init__(self):\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandAugment(num_ops=2, magnitude=9),\n",
        "            Cutout(mask_size=16, p=1.0),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                                 (0.247, 0.243, 0.261))\n",
        "        ])\n",
        "\n",
        "    def __call__(self, img):\n",
        "        return self.transform(img)\n",
        "\n",
        "# ---------- UDA Training Function ----------\n",
        "def train_uda(model, labelled_loader, unlabelled_loader, test_loader, device,\n",
        "              epochs=30, lambda_u=1.0, threshold=0.85):\n",
        "\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    strong_aug = StrongTransform()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_lab_loss = 0\n",
        "        total_unsup_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        unlab_iter = iter(unlabelled_loader)\n",
        "\n",
        "        for batch_idx, (lab_x, lab_y) in enumerate(labelled_loader):\n",
        "            try:\n",
        "                unlab_x, _ = next(unlab_iter)\n",
        "            except StopIteration:\n",
        "                unlab_iter = iter(unlabelled_loader)\n",
        "                unlab_x, _ = next(unlab_iter)\n",
        "\n",
        "            lab_x, lab_y = lab_x.to(device), lab_y.to(device)\n",
        "            unlab_x = unlab_x.to(device)\n",
        "\n",
        "            # Apply strong augmentation on unlabelled data\n",
        "            unlab_x_aug = torch.stack([strong_aug(img.cpu()) for img in unlab_x]).to(device)\n",
        "\n",
        "            # Supervised loss\n",
        "            logits_lab = model(lab_x)\n",
        "            # loss_lab = criterion(logits_lab, lab_y)\n",
        "            loss_lab = apply_tsa(logits_lab, lab_y, epoch, epochs, schedule='linear')\n",
        "\n",
        "\n",
        "            # Unsupervised loss with pseudo-label masking\n",
        "            with torch.no_grad():\n",
        "                logits_weak = model(unlab_x)\n",
        "                probs_weak = F.softmax(logits_weak, dim=1)\n",
        "                max_probs, pseudo_labels = torch.max(probs_weak, dim=1)\n",
        "                mask = max_probs.ge(threshold).float()\n",
        "\n",
        "            logits_strong = model(unlab_x_aug)\n",
        "            log_probs_strong = F.log_softmax(logits_strong, dim=1)\n",
        "            loss_unsup = F.kl_div(log_probs_strong, probs_weak, reduction='none').sum(dim=1)\n",
        "            loss_unsup = (loss_unsup * mask).mean()\n",
        "\n",
        "            loss = loss_lab + lambda_u * loss_unsup\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_lab_loss += loss_lab.item()\n",
        "            total_unsup_loss += loss_unsup.item()\n",
        "\n",
        "            _, preds = torch.max(logits_lab, 1)\n",
        "            correct += (preds == lab_y).sum().item()\n",
        "            total += lab_y.size(0)\n",
        "\n",
        "        acc = correct / total\n",
        "        print(f\"[Epoch {epoch+1}] Sup Loss: {total_lab_loss:.4f} | Unsup Loss: {total_unsup_loss:.4f} | Sup Acc: {acc:.4f}\")\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for x_test, y_test in test_loader:\n",
        "                x_test, y_test = x_test.to(device), y_test.to(device)\n",
        "                logits = model(x_test)\n",
        "                _, preds = torch.max(logits, 1)\n",
        "                correct += (preds == y_test).sum().item()\n",
        "                total += y_test.size(0)\n",
        "        print(f\"â†’ Test Acc: {correct / total:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# ---------- Main ----------\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    normalize = transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                                     (0.247, 0.243, 0.261))\n",
        "\n",
        "    weak_transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ])\n",
        "\n",
        "    full_train = datasets.CIFAR10(root='./data', train=True, download=True, transform=weak_transform)\n",
        "    test_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
        "\n",
        "    num_labelled = 4000\n",
        "    indices = np.arange(len(full_train))\n",
        "    np.random.seed(42)\n",
        "    np.random.shuffle(indices)\n",
        "    labelled_idx = indices[:num_labelled]\n",
        "    unlabelled_idx = indices[num_labelled:]\n",
        "\n",
        "    labelled_set = Subset(full_train, labelled_idx)\n",
        "    unlabelled_set = Subset(full_train, unlabelled_idx)\n",
        "\n",
        "    labelled_loader = DataLoader(labelled_set, batch_size=64, shuffle=True, num_workers=2, drop_last=True)\n",
        "    unlabelled_loader = DataLoader(unlabelled_set, batch_size=64, shuffle=True, num_workers=2, drop_last=True)\n",
        "    test_loader = DataLoader(test_set, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "    model = SimpleCNN(num_classes=10)\n",
        "    trained_model = train_uda(model, labelled_loader, unlabelled_loader, test_loader, device,\n",
        "                              epochs=30, lambda_u=1.0, threshold=0.85)\n",
        "\n",
        "    torch.save(trained_model.state_dict(), 'uda_cifar10_simplecnn_tsa.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtKZuUzoCRWw",
        "outputId": "47da08b3-ea22-4b45-ec52-c9833d935481"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] Sup Loss: 66.1398 | Unsup Loss: 0.0000 | Sup Acc: 0.1149\n",
            "â†’ Test Acc: 0.1211\n",
            "[Epoch 2] Sup Loss: 119.4171 | Unsup Loss: 0.0000 | Sup Acc: 0.1578\n",
            "â†’ Test Acc: 0.2561\n",
            "[Epoch 3] Sup Loss: 99.2260 | Unsup Loss: 0.0000 | Sup Acc: 0.2445\n",
            "â†’ Test Acc: 0.2976\n",
            "[Epoch 4] Sup Loss: 100.5792 | Unsup Loss: 0.0000 | Sup Acc: 0.2825\n",
            "â†’ Test Acc: 0.3261\n",
            "[Epoch 5] Sup Loss: 100.2116 | Unsup Loss: 0.0000 | Sup Acc: 0.3009\n",
            "â†’ Test Acc: 0.3588\n",
            "[Epoch 6] Sup Loss: 98.4903 | Unsup Loss: 0.0000 | Sup Acc: 0.3284\n",
            "â†’ Test Acc: 0.3539\n",
            "[Epoch 7] Sup Loss: 97.7126 | Unsup Loss: 0.0000 | Sup Acc: 0.3490\n",
            "â†’ Test Acc: 0.4014\n",
            "[Epoch 8] Sup Loss: 95.9460 | Unsup Loss: 0.0000 | Sup Acc: 0.3657\n",
            "â†’ Test Acc: 0.3883\n",
            "[Epoch 9] Sup Loss: 95.9225 | Unsup Loss: 0.0000 | Sup Acc: 0.3798\n",
            "â†’ Test Acc: 0.3940\n",
            "[Epoch 10] Sup Loss: 94.6920 | Unsup Loss: 0.0000 | Sup Acc: 0.3997\n",
            "â†’ Test Acc: 0.4389\n",
            "[Epoch 11] Sup Loss: 90.6933 | Unsup Loss: 0.0000 | Sup Acc: 0.4282\n",
            "â†’ Test Acc: 0.4689\n",
            "[Epoch 12] Sup Loss: 87.9350 | Unsup Loss: 0.0838 | Sup Acc: 0.4461\n",
            "â†’ Test Acc: 0.4657\n",
            "[Epoch 13] Sup Loss: 89.6433 | Unsup Loss: 0.0961 | Sup Acc: 0.4355\n",
            "â†’ Test Acc: 0.4770\n",
            "[Epoch 14] Sup Loss: 86.7237 | Unsup Loss: 0.5001 | Sup Acc: 0.4554\n",
            "â†’ Test Acc: 0.4872\n",
            "[Epoch 15] Sup Loss: 85.3097 | Unsup Loss: 0.2221 | Sup Acc: 0.4738\n",
            "â†’ Test Acc: 0.4762\n",
            "[Epoch 16] Sup Loss: 86.2449 | Unsup Loss: 0.9172 | Sup Acc: 0.4791\n",
            "â†’ Test Acc: 0.4747\n",
            "[Epoch 17] Sup Loss: 82.9634 | Unsup Loss: 0.4641 | Sup Acc: 0.4985\n",
            "â†’ Test Acc: 0.4706\n",
            "[Epoch 18] Sup Loss: 84.4982 | Unsup Loss: 1.1775 | Sup Acc: 0.4894\n",
            "â†’ Test Acc: 0.4669\n",
            "[Epoch 19] Sup Loss: 83.3241 | Unsup Loss: 1.5282 | Sup Acc: 0.4972\n",
            "â†’ Test Acc: 0.4994\n",
            "[Epoch 20] Sup Loss: 82.6415 | Unsup Loss: 2.0347 | Sup Acc: 0.5010\n",
            "â†’ Test Acc: 0.5056\n",
            "[Epoch 21] Sup Loss: 81.5324 | Unsup Loss: 2.2691 | Sup Acc: 0.5134\n",
            "â†’ Test Acc: 0.5067\n",
            "[Epoch 22] Sup Loss: 80.6708 | Unsup Loss: 2.6217 | Sup Acc: 0.5129\n",
            "â†’ Test Acc: 0.5107\n",
            "[Epoch 23] Sup Loss: 80.2958 | Unsup Loss: 3.2485 | Sup Acc: 0.5176\n",
            "â†’ Test Acc: 0.5101\n",
            "[Epoch 24] Sup Loss: 79.3633 | Unsup Loss: 3.9141 | Sup Acc: 0.5312\n",
            "â†’ Test Acc: 0.5200\n",
            "[Epoch 25] Sup Loss: 79.4882 | Unsup Loss: 5.0622 | Sup Acc: 0.5295\n",
            "â†’ Test Acc: 0.5188\n",
            "[Epoch 26] Sup Loss: 77.6427 | Unsup Loss: 4.9174 | Sup Acc: 0.5439\n",
            "â†’ Test Acc: 0.4971\n",
            "[Epoch 27] Sup Loss: 77.3518 | Unsup Loss: 5.5235 | Sup Acc: 0.5416\n",
            "â†’ Test Acc: 0.5125\n",
            "[Epoch 28] Sup Loss: 76.4590 | Unsup Loss: 7.0135 | Sup Acc: 0.5469\n",
            "â†’ Test Acc: 0.5176\n",
            "[Epoch 29] Sup Loss: 77.3579 | Unsup Loss: 6.5263 | Sup Acc: 0.5441\n",
            "â†’ Test Acc: 0.5368\n",
            "[Epoch 30] Sup Loss: 76.9705 | Unsup Loss: 6.9369 | Sup Acc: 0.5441\n",
            "â†’ Test Acc: 0.5353\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y4o7qmjnGmjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def evaluate(model, test_loader, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in test_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            logits = model(x)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            all_preds.append(preds.cpu())\n",
        "            all_labels.append(y.cpu())\n",
        "\n",
        "    # Flatten predictions and labels\n",
        "    y_pred = torch.cat(all_preds).numpy()\n",
        "    y_true = torch.cat(all_labels).numpy()\n",
        "\n",
        "    # Compute accuracy\n",
        "    accuracy = np.mean(y_pred == y_true)\n",
        "    print(f\"\\nâœ… Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "    # Classification report\n",
        "    print(\"\\nðŸ“Š Classification Report:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=[\n",
        "        'airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "        'dog', 'frog', 'horse', 'ship', 'truck'\n",
        "    ]))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=[\n",
        "                    'airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "                    'dog', 'frog', 'horse', 'ship', 'truck'\n",
        "                ],\n",
        "                yticklabels=[\n",
        "                    'airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "                    'dog', 'frog', 'horse', 'ship', 'truck'\n",
        "                ])\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "N0s2MK1sE3iO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SimpleCNN(num_classes=10)\n",
        "model.load_state_dict(torch.load('uda_cifar10_simplecnn_tsa.pth'))\n",
        "model.to(device)\n",
        "evaluate(model, test_loader, device)"
      ],
      "metadata": {
        "id": "WMwRVKTHFAp0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}